# PLM-ICD-multi-label-classifier
A non-official multi-label classifier based on [PLM-ICD paper](https://arxiv.org/abs/2207.05289). 

Basically this is my personal side project. The target is deep understanding 
[paper](https://arxiv.org/abs/2207.05289). Finally, here provide a more concise 
and clear implementation, which can make things easier when need do some 
custimization or extension.


## Usage
### Python Env
```sh
python -m venv ./_venv --copies
source ./_venv/bin/activate
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
# deactivate
```
### Run Tests
```sh
python -m pytest ./test --cov=./src/plm_icd_multi_label_classifier --durations=0 -v
```

### Data
```sh
python etl_mimic3_processing.py ${YOUR_MIMIC3_DATA_DIRECTORY}
```

### Training and Evaluation
```sh
CUDA_VISIBLE_DEVICES=0,1,2,3 python ./train.py train.json
```
#### Training Config File
The format should be JSON, most of parameters are easy to understand is your are a 
MLE or researcher:
* `chunk_size`: Each chunks token ID number.
* `chunk_num`: The number of chunk each text/document should have, padding first for short sentences.
* `hf_lm`: HuggingFace language model name/path.
* `lm_hidden_dim`: Language model's hidden output layer's dimension.
* `data_dir`: Data directory, should at least contains two files generated by `etl_mimic3_processing.py`:
  * train.jsonl
  * dev.jsonl
* `training_engine`: Training engine, can be "torch" or "ray". Torch mode is mainly used for debugging purpose and not supporting distributed training.
* `single_worker_batch_size`: Each worker's batch size. Note if training with "torch" engine, then only have one worker.
* `lr`: Initial learning rate.
* `epochs`: Training epochs.
* `gpu`: If using GPU to train.
* `workers`: Eorkers number in distrubued training. This is only effective when using "ray" as training engine.
* `single_worker_eval_size`: Each worker's maximum evaluation sample size. Again when using "torch" as training engine, you only have one worker.
* `random_seed`: Random seed, this can make sure you can 100% reproduce training.
* `text_col`: Text column name in train/dev/test JSON line dataset.
* `label_col`: Label column name in train/dev/test JSON line dataset.
* `ckpt_dir`: Checkpoint directory name.
* `log_period`: How many **batchs** passed before each time's evaluation log printing.
* `dump_period`: How many **steps** passed before each time's checkpoint dumping.

## Other Implementation Details
* After `chunk_size` and `chunk_num` defined, each text's token ID length are fixed to `chunk_size * chunk_num`. 
if not long enough then automatically padding first.

