# PLM-ICD-multi-label-classifier
A non-official multi-label classifier based on [PLM-ICD paper](https://arxiv.org/abs/2207.05289). 

Basically this is my personal side project. The target is deep understanding 
[paper](https://arxiv.org/abs/2207.05289). Finally, here provide a more concise 
and clear implementation, which can make things easier when need do some 
custimization or extension.
 
Although the model comes from [paper](https://arxiv.org/abs/2207.05289), I tried my best 
to make this as a general program for text multi-label classification task.

## Usage
### Python Env
```sh
python -m venv ./_venv --copies
source ./_venv/bin/activate
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
# deactivate
```
### Run Tests
```sh
python -m pytest ./test --cov=./src/plm_icd_multi_label_classifier --durations=0 -v
```

### Data
The data should be in JSON line format, here provide an MIMIC-III data ETL program:
```sh
python etl_mimic3_processing.py ${YOUR_MIMIC3_DATA_DIRECTORY} ${YOUR_TARGET_OUTPUT_DIRECTORY}
```
When you need use this program do text multi-label classification on your custimized 
data set, you can just transfer it into a JSON line file, and using **training config** 
file to specify which field is text and which is label. 

**NOTE**, since here you are dealing a multi-label classification task, the format of 
label field should be as a CSV string, for example:
```
{"text": "this is a fake text.", "label": "label1,label2,label3,label4"}
```

### Training and Evaluation
```sh
CUDA_VISIBLE_DEVICES=0,1,2,3 python ./train.py ${TRAIN_CONFIG_JSON_FILE_PATH}
```

#### Training Config File
The format should be JSON, most of parameters are easy to understand is your are a 
MLE or researcher:
* `chunk_size`: Each chunks token ID number.
* `chunk_num`: The number of chunk each text/document should have, padding first for short sentences.
* `hf_lm`: HuggingFace language model name/path.
* `lm_hidden_dim`: Language model's hidden output layer's dimension.
* `data_dir`: Data directory, should at least contains two files generated by `etl_mimic3_processing.py`:
  * train.jsonl
  * dev.jsonl
* `training_engine`: Training engine, can be "torch" or "ray". Torch mode is mainly used for debugging purpose and not supporting distributed training.
* `single_worker_batch_size`: Each worker's batch size. Note if training with "torch" engine, then only have one worker.
* `lr`: Initial learning rate.
* `epochs`: Training epochs.
* `gpu`: If using GPU to train.
* `workers`: Eorkers number in distrubued training. This is only effective when using "ray" as training engine.
* `single_worker_eval_size`: Each worker's maximum evaluation sample size. Again when using "torch" as training engine, you only have one worker.
* `random_seed`: Random seed, this can make sure you can 100% reproduce training.
* `text_col`: Text column name in train/dev/test JSON line dataset.
* `label_col`: Label column name in train/dev/test JSON line dataset.
* `ckpt_dir`: Checkpoint directory name.
* `log_period`: How many **batchs** passed before each time's evaluation log printing.
* `dump_period`: How many **steps** passed before each time's checkpoint dumping.

## Other Implementation Details
* After `chunk_size` and `chunk_num` defined, each text's token ID length are fixed to `chunk_size * chunk_num`. 
if not long enough then automatically padding first.

